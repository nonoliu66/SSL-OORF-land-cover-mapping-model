# -*- coding: utf-8 -*-
"""æ ·æœ¬ç­›é€‰ä¸é‡è¦æ€§åˆ†æ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aE0weOfKN7YQCGSL08v6Y6gfVtI7lTuE
"""

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd

# è¯»å–CSVæ–‡ä»¶
df1 = pd.read_csv('/content/drive/My Drive/SamplePoints_2019_2023.csv')

filtered_df = df1[(df1['sadImg'] > 0.992) & (df1['edImg'] < 0.01)]

# 3. æŸ¥çœ‹ç»“æœ
print(filtered_df.head())
print(f"ç­›é€‰åæ ·æœ¬æ•°é‡ï¼š{len(filtered_df)}")

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd

# Calculate the average over ten trials
print(filtered_df.columns.tolist())
X = filtered_df[['blue','green','red','nir','swir1','swir2']]
y = filtered_df['b1']
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import (
    precision_score,
    recall_score,
    f1_score,
    accuracy_score,
    cohen_kappa_score
)
# -----------------------------
# 3. åˆ’åˆ†è®­ç»ƒä¸æµ‹è¯•é›†
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# -----------------------------
# 4. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
# -----------------------------
rf = RandomForestClassifier(
    n_estimators=100,        # æ ‘æ•°é‡
    max_depth=None,          # ä¸é™åˆ¶æ·±åº¦
    min_samples_split=2,
    min_samples_leaf=1,
    random_state=42,
    n_jobs=-1
)

rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
kappa = cohen_kappa_score(y_test, y_pred)
# -----------------------------
# 5. æ¨¡å‹é¢„æµ‹ä¸è¯„ä¼°
# -----------------------------
print("\nâœ… æ¨¡å‹å‡†ç¡®ç‡ï¼š", accuracy_score(y_test, y_pred))
print("\nåˆ†ç±»æŠ¥å‘Šï¼š\n", classification_report(y_test, y_pred))
print("\næ··æ·†çŸ©é˜µï¼š\n", confusion_matrix(y_test, y_pred))
print(f"Kappa:             {kappa:.4f}")

import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
# å¦‚æœä½ çš„åˆ—åä¸åŒï¼Œæ¯”å¦‚ 'lon', 'lat'ï¼Œè¯·æ”¹æˆå¯¹åº”åˆ—å
lon_col = 'longitude'
lat_col = 'latitude'

if lon_col not in filtered_df.columns or lat_col not in filtered_df.columns:
    raise ValueError("âš ï¸ CSVæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ° 'longitude' å’Œ 'latitude' åˆ—ï¼Œè¯·æ£€æŸ¥åˆ—åï¼")

# 4. è½¬æ¢ä¸º GeoDataFrameï¼ˆæ¯ä¸ªæ ·æœ¬ç‚¹éƒ½æœ‰ç©ºé—´åæ ‡ï¼‰
gdf = gpd.GeoDataFrame(
    filtered_df,
    geometry=gpd.points_from_xy(filtered_df[lon_col], filtered_df[lat_col]),
    crs="EPSG:4326"  # WGS84åæ ‡ç³»ï¼ŒGEEè¦æ±‚æ­¤åæ ‡ç³»
)

# 5. åªä¿ç•™ class å±æ€§å’Œå‡ ä½•ä¿¡æ¯ï¼ˆå¦‚æœä½ åªæƒ³ä¿ç•™è¿™äº›å­—æ®µï¼‰
gdf_export = gdf[['b1', 'geometry']]

# 6. å¯¼å‡ºä¸º shapefileï¼ˆå››ä¸ªæ–‡ä»¶ï¼š.shp/.shx/.dbf/.prjï¼‰
output_path = "filtered_points.shp"
gdf_export.to_file(output_path, driver='ESRI Shapefile', encoding='utf-8')

print("âœ… æˆåŠŸå¯¼å‡ºï¼æ–‡ä»¶è·¯å¾„ï¼š", output_path)
print("âœ”ï¸ å¯åœ¨ GEE ä¸­ Assets â†’ NEW â†’ Table â†’ ä¸Šä¼  filtered_points.shpï¼ˆåŠå…¶é…å¥—æ–‡ä»¶ï¼‰")
import shutil

# å‹ç¼©ä¸º ZIP æ–‡ä»¶ï¼ˆGEE ä¸Šä¼ æ—¶æ›´æ–¹ä¾¿ï¼‰
shutil.make_archive('/content/filtered_points', 'zip', '/content', 'filtered_points.shp')

from google.colab import files
files.download('/content/filtered_points.zip')

# ==========================================================
# âœ… å°†ç­›é€‰ç»“æœå¯¼å‡ºä¸º shapefile å¹¶ä¸‹è½½ï¼ˆåŒ…å« class å±æ€§ï¼‰
# ==========================================================

import geopandas as gpd
import os
import shutil
from google.colab import files
filtered_df = filtered_df[['b1', 'longitude', 'latitude']]
# å‡è®¾ df_filtered å·²ç»å­˜åœ¨ï¼ŒåŒ…å« 'longitude', 'latitude', 'class' ç­‰åˆ—
print("æ ·ä¾‹æ•°æ®ï¼š")
print(filtered_df.head())

# ==========================================================
# 1. å°† DataFrame è½¬ä¸º GeoDataFrame
# ==========================================================
gdf_export = gpd.GeoDataFrame(
    filtered_df,
    geometry=gpd.points_from_xy(gpd.points_from_xy(df_simple.longitude, df_simple.latitude)),
    crs="EPSG:4326"
)

# ==========================================================
# 2. ä¿å­˜ä¸º Shapefile
# ==========================================================
out_dir = "/content/shapefile_export"
os.makedirs(out_dir, exist_ok=True)

output_path = os.path.join(out_dir, "filtered_points.shp")
gdf_export.to_file(output_path, driver="ESRI Shapefile")

print("\nâœ… å¯¼å‡ºçš„ shapefile æ–‡ä»¶åˆ—è¡¨ï¼š")
for f in os.listdir(out_dir):
    print(" -", f)

# ==========================================================
# 3. ä»…æ‰“åŒ…è¯¥ç›®å½•ä¸‹çš„ shapefile æ–‡ä»¶
# ==========================================================
zip_path = "/content/filtered_points.zip"
shutil.make_archive('/content/filtered_points', 'zip', out_dir)

# ==========================================================
# 4. ä¸‹è½½ ZIP æ–‡ä»¶ï¼ˆåŒ…å« .shp, .shx, .dbf, .prj ç­‰ï¼‰
# ==========================================================
print("\nğŸ“¦ æ‰“åŒ…å®Œæˆï¼Œå¼€å§‹ä¸‹è½½...")
files.download(zip_path)
print("âœ… ä¸‹è½½å®Œæˆï¼å¯ç›´æ¥ä¸Šä¼ åˆ° GEEã€‚")

import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
import zipfile, os

# === 1. ä»…ä¿ç•™ classã€longitudeã€latitude ä¸‰åˆ— ===
df_simple = filtered_df[['b1', 'longitude', 'latitude']].copy()

# æ£€æŸ¥åˆ—åï¼Œç¡®ä¿æ²¡æœ‰ä¿ç•™å…³é”®å­—
df_simple.columns = ['b1', 'longitude', 'latitude']

# === 2. åˆ›å»º GeoDataFrame ===
gdf = gpd.GeoDataFrame(
    df_simple,
    geometry=gpd.points_from_xy(df_simple.longitude, df_simple.latitude),
    crs="EPSG:4326"
)

# === 3. å»æ‰ç´¢å¼•å’Œä¿ç•™å¹²å‡€å­—æ®µ ===
# æ³¨æ„ï¼šGEE ä¸å…è®¸ä¸Šä¼  geometry åç§°ä»¥å¤–çš„éšè—åˆ—
gdf = gdf[['b1', 'geometry']].copy()

# === 4. ä¿å­˜ä¸º Shapefile ===
output_dir = '/content/filtered_points_shp'
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, 'filtered_points.shp')

gdf.to_file(output_path, driver='ESRI Shapefile', index=False)

# === 5. æ‰“åŒ…ä¸º zip æ–‡ä»¶ ===
shp_zip = '/content/filtered_points.zip'
with zipfile.ZipFile(shp_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:
    for root, dirs, files in os.walk(output_dir):
        for file in files:
            zipf.write(os.path.join(root, file),
                       arcname=os.path.relpath(os.path.join(root, file), output_dir))

print("âœ… Shapefile å·²å¯¼å‡ºæˆåŠŸï¼")
print("ğŸ“‚ æ–‡ä»¶è·¯å¾„:", shp_zip)
print("è¯·åœ¨å·¦ä¾§æ–‡ä»¶æ ä¸­å³é”®ä¸‹è½½ filtered_points.zip è¿›è¡Œä¸Šä¼ ã€‚")
from google.colab import files

# æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
!ls -lh /content/

# å¦‚æœè¦ç›´æ¥ä¸‹è½½ ZIP æ–‡ä»¶
files.download("/content/filtered_points.zip")

# =====================================
# âœ… åœ¨ Colab ä¸­å®‰è£… Times New Roman å­—ä½“
# =====================================
!apt-get install -y msttcorefonts -qq > /dev/null

import matplotlib.pyplot as plt
import matplotlib as mpl

# è®¾ç½®å…¨å±€å­—ä½“ä¸º Times New Roman
mpl.rcParams['font.family'] = 'Times New Roman'
mpl.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

print("âœ… Times New Roman å­—ä½“å·²å¯ç”¨ã€‚")

# ===============================
# 1. å¯¼å…¥ä¾èµ–åº“
# ===============================
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
import shap
from google.colab import files
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score
import matplotlib.pyplot as plt
import seaborn as sns
import shap
from google.colab import files
# ===============================
# 2. ä¸Šä¼ å¹¶è¯»å– CSV æ–‡ä»¶
# ===============================


df = pd.read_csv('/content/drive/My Drive/Sample_Features_2023117.csv')
print(df.head())

# ===============================
# 3. æŒ‡å®šç±»åˆ«åˆ—å¹¶åˆ†ç¦»ç‰¹å¾ä¸æ ‡ç­¾
# ===============================
# âš ï¸ ä¿®æ”¹è¿™é‡Œçš„ 'class' ä¸ºä½ çš„ç±»åˆ«åˆ—å
label_col = 'b1'

X = df.drop(columns=['b1','system:index','.geo','NDVIthr','NDVImax','L_integral'])
y = df[label_col]

# ===============================
# 4. åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†
# ===============================
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ===============================
# 5. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
# ===============================
rf = RandomForestClassifier(
    n_estimators=500,
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train)

print("âœ… éšæœºæ£®æ—æ¨¡å‹è®­ç»ƒå®Œæˆ")

# 6. ç‰¹å¾é‡è¦æ€§è®¡ç®—
# ===============================
importances = rf.feature_importances_
fi_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})
fi_df = fi_df.sort_values('Importance', ascending=False)

# ===============================
# 7. å¤šæŒ‡æ ‡ç²¾åº¦åˆ†æ
# ===============================
metrics_list = []
feature_counts = range(1, len(fi_df)+1)

for k in feature_counts:
    selected_features = fi_df['Feature'].iloc[:k].tolist()
    rf_k = RandomForestClassifier(n_estimators=500, random_state=42, n_jobs=-1)
    rf_k.fit(X_train[selected_features], y_train)
    y_pred = rf_k.predict(X_test[selected_features])

    acc = accuracy_score(y_test, y_pred)
    pre = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    kappa = cohen_kappa_score(y_test, y_pred)

    metrics_list.append({
        'Feature_Count': k,
        'Accuracy': acc,
        'Precision': pre,
        'Recall': rec,
        'F1_Score': f1,
        'Kappa': kappa
    })

# è½¬æ¢ä¸º DataFrame
metrics_df = pd.DataFrame(metrics_list)



# ===============================
# 9. ä¿å­˜ç²¾åº¦æŒ‡æ ‡ç»“æœ
# ===============================
metrics_df.to_csv('randomforest_feature_metrics.csv', index=False)
files.download('randomforest_feature_metrics.csv')
print("ğŸ“¤ å·²å¯¼å‡º randomforest_feature_metrics.csv")

# ===============================
# 10. å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§ï¼ˆTop 45ï¼‰
# ===============================
plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=fi_df.head(44))
plt.title('Top 45 Feature Importances (Random Forest)')
plt.tight_layout()
plt.show()

# ===============================
# 7. å¤šæŒ‡æ ‡ç²¾åº¦åˆ†æ + è¢‹å¤–è¯¯å·®(OOB Error)
# ===============================
metrics_list = []
feature_counts = range(1, len(fi_df)+1)

for k in feature_counts:
    selected_features = fi_df['Feature'].iloc[:k].tolist()
    rf_k = RandomForestClassifier(
        n_estimators=100,
        random_state=42,
        n_jobs=-1,
        oob_score=True  # âœ… å¯ç”¨è¢‹å¤–ä¼°è®¡
    )
    rf_k.fit(X_train[selected_features], y_train)

    # è¢‹å¤–åˆ†æ•°ä¸è¯¯å·®
    oob_score = rf_k.oob_score_
    oob_error = 1 - oob_score

    # æµ‹è¯•é›†é¢„æµ‹ç²¾åº¦
    y_pred = rf_k.predict(X_test[selected_features])

    acc = accuracy_score(y_test, y_pred)
    pre = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    kappa = cohen_kappa_score(y_test, y_pred)

    metrics_list.append({
        'Feature_Count': k,
        'Accuracy': acc,
        'Precision': pre,
        'Recall': rec,
        'F1_Score': f1,
        'Kappa': kappa,
        'OOB_Score': oob_score,
        'OOB_Error': oob_error
    })

# è½¬æ¢ä¸º DataFrame
metrics_df = pd.DataFrame(metrics_list)



# ===============================
# 9. ä¿å­˜ç²¾åº¦æŒ‡æ ‡ç»“æœï¼ˆåŒ…å«è¢‹å¤–è¯¯å·®ï¼‰
# ===============================
metrics_df.to_csv('randomforest_feature_oob_metrics.csv', index=False)
files.download('randomforest_feature_oob_metrics.csv')
print("ğŸ“¤ å·²å¯¼å‡º randomforest_feature_oob_metrics.csv")

# å¯¼å‡ºå¹¶ä¸‹è½½ç‰¹å¾é‡è¦æ€§æ’åº
fi_df.to_csv('feature_importance_randomforest.csv', index=False)
files.download('feature_importance_randomforest.csv')
print("ğŸ“¤ å·²å¯¼å‡º feature_importance_randomforest.csv")

# ===============================
# 1. å¯¼å…¥ä¾èµ–åº“
# ===============================
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
import shap
from google.colab import files
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
df = pd.read_csv('/content/drive/My Drive/Sample_Features_2023.csv')
X = df[['Peak_val','Base_val','B2','MNDWI','NDVI','RVI','End_val','B12','VH','B3','B4','VV','Start_val','B11','NDWI',]]
y = df['b1']
# æ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=None,
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_train, y_train)

import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

# å­—ä½“è·¯å¾„
font_path = '/content/drive/My Drive/TIMES.TTF'

# æ³¨å†Œå­—ä½“
fm.fontManager.addfont(font_path)

# è®¾ç½® Matplotlib å…¨å±€å­—ä½“
plt.rcParams['font.family'] = 'Times New Roman'

import shap
explainer = shap.TreeExplainer(rf_model)
# è®¡ç®—æµ‹è¯•é›†çš„shapå€¼
shap_values = explainer.shap_values(X_train)
plt.rcParams.update({
    'font.family': 'Times New Roman',   # ä½¿ç”¨ Times New Roman
    'font.size': 124,                    # é»˜è®¤å­—ä½“å¤§å°
    'axes.titlesize': 26,               # å›¾æ ‡é¢˜å­—ä½“å¤§å°
    'axes.labelsize': 124,               # åæ ‡è½´æ ‡ç­¾å­—ä½“å¤§å°
    'xtick.labelsize': 122,              # X è½´åˆ»åº¦
    'ytick.labelsize': 122,              # Y è½´åˆ»åº¦
    'legend.fontsize': 122,              # å›¾ä¾‹å­—ä½“
    'figure.titlesize': 26              # å…¨å±€æ ‡é¢˜
})
# æ¯”å¦‚ç»˜åˆ¶ç¬¬ 0 ç±»ï¼ˆç±»åˆ«ç´¢å¼•ä¸º 0ï¼‰
shap.summary_plot(shap_values[:, :, 0], X_train, feature_names=X_train.columns)
shap.summary_plot(shap_values[:, :, 1], X_train, feature_names=X_train.columns)
shap.summary_plot(shap_values[:, :, 2], X_train, feature_names=X_train.columns)
shap.summary_plot(shap_values[:, :, 3], X_train, feature_names=X_train.columns)
shap.summary_plot(shap_values[:, :, 4], X_train, feature_names=X_train.columns)
shap.summary_plot(shap_values[:, :, 5], X_train, feature_names=X_train.columns)
shap.summary_plot(shap_values[:, :, 6], X_train, feature_names=X_train.columns)
shap.summary_plot(shap_values, X_train, feature_names=X_train.columns,max_display=len(X_train.columns))
shap.summary_plot(shap_values, X_train, plot_type="bar", show=False)
plt.title("SHAP Feature Importance (Top 18 Features)")
plt.tight_layout()
plt.show()

shap.summary_plot(shap_values, X_train, feature_names=X_train.columns,max_display=len(X_train.columns))

import shap
import matplotlib.pyplot as plt
import os

# å‡è®¾ shap_values æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [n_samples, n_features, n_classes] çš„ä¸‰ç»´æ•°ç»„
# X_train æ˜¯ä½ çš„ç‰¹å¾æ•°æ® DataFrame
# è¿™é‡Œä»¥ 7 ä¸ªç±»åˆ«ä¸ºä¾‹
num_classes = shap_values.shape[2]

# åˆ›å»ºä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
os.makedirs("shap_plots", exist_ok=True)

# å¾ªç¯ç»˜åˆ¶æ¯ä¸ªç±»åˆ«çš„ SHAP summary å›¾
for i in range(num_classes):
    plt.figure(figsize=(7, 7))
    shap.summary_plot(
        shap_values[:, :, i],
        X_train,
        feature_names=X_train.columns,
        show=False  # ä¸ç«‹å³å±•ç¤º
    )
    plt.title(f"SHAP Feature Impact for Class {i}", fontsize=14, fontweight='bold', fontname='Times New Roman')
    plt.tight_layout()
    plt.savefig(f"shap_plots/SHAP_Class{i}.png", dpi=300, bbox_inches='tight')
    plt.close()

import shap
import matplotlib.pyplot as plt
import shap

plt.rcParams.update({
    'font.family': 'Times New Roman',   # ä½¿ç”¨ Times New Roman
    'font.size': 100,                    # é»˜è®¤å­—ä½“å¤§å°
    'axes.titlesize': 20,               # å›¾æ ‡é¢˜å­—ä½“å¤§å°
    'axes.labelsize': 100,               # åæ ‡è½´æ ‡ç­¾å­—ä½“å¤§å°
    'xtick.labelsize': 100,              # X è½´åˆ»åº¦
    'ytick.labelsize': 100,              # Y è½´åˆ»åº¦
    'legend.fontsize': 15,              # å›¾ä¾‹å­—ä½“
    'figure.titlesize': 5              # å…¨å±€æ ‡é¢˜
})
shap.summary_plot(shap_values, X_train, plot_type="bar", show=False)
plt.title("")

# è·å–å½“å‰å›¾ä¾‹
handles, labels = plt.gca().get_legend_handles_labels()

# è‡ªå®šä¹‰ legend æ’åº (ä¾‹å¦‚æŒ‰ Class 1â€“7)
order = sorted(range(len(labels)), key=lambda i: int(labels[i].split()[-1]))  # è‡ªåŠ¨æŒ‰æ•°å­—æ’åº
plt.legend([handles[i] for i in order], [labels[i] for i in order])
plt.title("SHAP Feature Importance (Top 15 Features)")
plt.tight_layout()
plt.show()